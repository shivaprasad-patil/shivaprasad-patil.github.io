---
title: "ScAdver: Adversarial Batch Correction for Single-Cell Data"
collection: projects
excerpt: "Train once, project forever: Adversarial batch correction for single-cell RNA-seq that preserves biology while removing batch effects with fast inference (<1s per query batch)."
date: 2025-08-30
github: "https://github.com/shivaprasad-patil/ScAdver"
header:
  teaser: /images/scadver-thumb.png
---

# ðŸ§¬ ScAdver â€” Adversarial Batch Correction for Single-Cell Data

ScAdver performs adversarial batch correction for single-cell RNA-seq data, eliminating technical batch effects while preserving biological variation and cell type identity. The framework features a train-once, project-forever paradigmâ€”train on reference data, then rapidly transform unlimited query batches without retraining. For challenging scenarios with large protocol shifts (e.g., 10X â†’ Smart-seq2), advanced domain adaptation via residual adapters enables robust cross-technology integration while maintaining biological fidelity.

![ðŸ”§ Architecture](/files/ScAdver_workflow.png)

## Key Features

- âœ… **Train once, project forever**: Save trained model and process unlimited query batches
- âš¡ **Fast inference**: No retraining required
- ðŸŽ¯ **Biology preserved**: Cell types and biological variation maintained
- ðŸ”„ **Batch-free**: Technical variation and protocol effects removed
- ðŸ–¥ï¸ **Multi-device**: Supports CPU, CUDA, and Apple Silicon (MPS)

## Installation

```bash
pip install git+https://github.com/shivaprasad-patil/ScAdver.git
```

## Quick Start

### Basic Usage

```python
import scanpy as sc
from scadver import adversarial_batch_correction

# Load data
adata = sc.read("your_data.h5ad")

# Run batch correction
adata_corrected, model, metrics = adversarial_batch_correction(
    adata=adata,
    bio_label='celltype',
    batch_label='batch',
    epochs=500
)

# Visualize
sc.pp.neighbors(adata_corrected, use_rep='X_ScAdver')
sc.tl.umap(adata_corrected)
sc.pl.umap(adata_corrected, color=['celltype', 'batch'])
```

### Query Processing

ScAdver uses a unified projection approach with optional residual adapters:

```python
import torch
from scadver import adversarial_batch_correction, transform_query_adaptive

# Step 1: Train on reference (once)
adata_ref_corrected, model, metrics = adversarial_batch_correction(
    adata=adata_reference,
    bio_label='celltype',
    batch_label='tech',
    epochs=500
)

# Step 2: Save model
torch.save(model.state_dict(), 'scadver_model.pt')

# Step 3a: Fast projection (adapter_dim=0)
adata_query1 = transform_query_adaptive(model, adata_query_batch1)
adata_query2 = transform_query_adaptive(model, adata_query_batch2)
# ... unlimited batches

# Step 3b: Adaptive projection (for domain shifts, adapter_dim>0)
adata_query_adapted = transform_query_adaptive(
    model=model,
    adata_query=adata_query,
    adata_reference=adata_reference[:500],  # Small reference sample
    bio_label='celltype',  # Optional: enables supervised adaptation
    adapter_dim=128,  # Enable residual adapter
    adaptation_epochs=50
)

# Step 4: Combine and analyze
adata_all = sc.concat([adata_ref_corrected, adata_query1, adata_query2])
sc.pp.neighbors(adata_all, use_rep='X_ScAdver')
sc.tl.umap(adata_all)
```

**Unified Approach with Residual Adapters**:

**Fast Mode (adapter_dim=0, default)**:
- âš¡ Direct projection through frozen encoder
- âœ… No adaptation needed: adapter mathematically reduces to zero
- âœ… Perfect for similar protocols/technologies
- âœ… Scales to unlimited query batches

**Adaptive Mode (adapter_dim>0)**:
- ðŸ”¬ Residual adapter handles domain shifts
- âœ… Better for protocol differences (e.g., 10X â†’ Smart-seq2)
- âœ… Optional biological supervision for improved alignment
- âš ï¸ Slower: trains small adapter network (~50 epochs)
- âœ… Adapter learns to be near-zero if no adaptation needed

**Key Insight**: When `adapter_dim>0` but query is similar to reference, the adapter automatically learns to stay close to zero, making it equivalent to fast mode. This makes the framework robust and adaptive to the data's needs.

## Usage Workflows

ScAdver offers **two flexible workflows** depending on your use case:

### Workflow 1: All-in-One (Process Everything Together)

Train and correct all data in a single call. The model trains only on reference data but corrects both reference and query:

```python
import numpy as np
from scadver import adversarial_batch_correction

# Mark which samples are Reference vs Query
query_mask = np.array([tech in ["smartseq2", "celseq2"] for tech in adata.obs["tech"]])
adata.obs['Source'] = np.where(query_mask, "Query", "Reference")

# Train on Reference, correct both Reference and Query
adata_corrected, model, metrics = adversarial_batch_correction(
    adata=adata,  # All data
    bio_label='celltype',
    batch_label='tech',
    reference_data='Reference',  # Model trains ONLY on these samples
    query_data='Query',          # Corrected but doesn't influence training
    epochs=500
)
```

**âœ… Use when:**
- You have all data available upfront
- One-time batch correction workflow
- Want to process everything in a single step

**Example**: See [examples/pancreas_example.py](examples/pancreas_example.py)

### Workflow 2: Train-Then-Project (Reusable Model)

Train once on reference, then project unlimited query batches as they arrive:

```python
import torch
from scadver import adversarial_batch_correction, transform_query_adaptive

# Step 1: Train on reference (once)
adata_ref_corrected, model, metrics = adversarial_batch_correction(
    adata=adata_reference,
    bio_label='celltype',
    batch_label='tech',
    epochs=500
)

# Step 2: Save model for reuse
torch.save(model.state_dict(), 'scadver_model.pt')

# Step 3: Project queries as they arrive (unlimited batches!)
# Fast mode (< 1 second per batch)
adata_query1 = transform_query_adaptive(model, query_batch1)
adata_query2 = transform_query_adaptive(model, query_batch2)
adata_query3 = transform_query_adaptive(model, query_batch3)

# Adaptive mode (for domain shifts)
adata_query4 = transform_query_adaptive(
    model, query_batch4,
    adata_reference=adata_ref[:500],
    adapter_dim=128,
    adaptation_epochs=50
)
```

**âœ… Use when:**
- Query batches arrive over time (streaming data)
- Want to reuse the same model for many queries
- Deploying model as a service

**Example**: See [examples/query_projection_notebook.ipynb](examples/query_projection_notebook.ipynb)

### Which Workflow to Choose?

| Scenario | Recommended Workflow |
|----------|---------------------|
| All data available now, one-time analysis | **Workflow 1** (All-in-One) |
| Query batches arrive over time | **Workflow 2** (Train-Then-Project) |
| Need to process 100+ query batches | **Workflow 2** (Train-Then-Project) |
| Deploying as a service | **Workflow 2** (Train-Then-Project) |
| Interactive analysis, have all data | **Workflow 1** (All-in-One) |

## How It Works

The encoder learns to:
- âœ… Keep biological patterns (via bio-classifier)
- âŒ Remove batch patterns (via adversarial discriminator)

Once trained, the frozen encoder automatically applies this transformation to new dataâ€”no ining needed.

## Output

- **Latent embeddings**: `adata.obsm['X_ScAdver']` (256-dimensional, batch-corrected)
- **Reconstructed expression**: `adata.layers['ScAdver_reconstructed']` (optional, use `return_reconstructed=True`)
- **Metrics**: Biology preservation, batch correction, overall score

## Documentation

- **[ENCODER_MECHANISM_EXPLAINED.md](ENCODER_MECHANISM_EXPLAINED.md)** - How the encoder training and projection works
- **[RESIDUAL_ADAPTER.md](RESIDUAL_ADAPTER.md)** - Residual adapters for domain adaptation
- **[Query Projection Notebook](examples/query_projection_notebook.ipynb)** - Fast vs adaptive comparison

---

**ðŸ“‚ GitHub Repository:** [ScAdver](https://github.com/shivaprasad-patil/ScAdver)  